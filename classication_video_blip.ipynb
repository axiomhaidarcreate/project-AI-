{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axiomhaidarcreate/project-AI-/blob/main/classication_video_blip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mf6NB7_kCb2U",
        "outputId": "2a305ade-3abe-44f4-8885-6d77b742be8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ UCF101 ZIP ...\n",
            "Downloading...\n",
            "From: https://huggingface.co/datasets/quchenyuan/UCF101-ZIP/resolve/main/UCF-101.zip\n",
            "To: /content/drive/MyDrive/UCF101_project/UCF-101.zip\n",
            "100% 6.96G/6.96G [01:29<00:00, 77.7MB/s]\n",
            "ğŸ“‚ ÙÙƒ Ø¶ØºØ· UCF101 ...\n",
            "âœ… ØªÙ… ÙÙƒ Ø§Ù„Ø¶ØºØ·ØŒ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ: /content/drive/MyDrive/UCF101_project/UCF-101\n",
            "âœ… ØªÙ… Ù†Ø³Ø® Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ù…Ø®ØªØ§Ø±Ø© Ø¥Ù„Ù‰: /content/drive/MyDrive/UCF101_project/videos_subset\n",
            "âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ZIP: /content/drive/MyDrive/UCF101_project/videos_subset.zip\n",
            "ğŸ’¾ Ø­Ø¬Ù… Ø§Ù„Ù…Ø¬Ù„Ø¯ videos_subset: 0.23 GB\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# STEP 0: ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n",
        "# ========================================\n",
        "!pip install opencv-python tqdm gdown --quiet\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ========================================\n",
        "# STEP 1: Ø±Ø¨Ø· Google Drive\n",
        "# ========================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ù…Ø³Ø§Ø± Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¯Ø±Ø§ÙŠÙ (ÙŠÙ…ÙƒÙ† ØªØºÙŠÙŠØ±Ù‡ Ù„Ùˆ ØªØ­Ø¨)\n",
        "base_drive_path = \"/content/drive/MyDrive/UCF101_project\"\n",
        "os.makedirs(base_drive_path, exist_ok=True)\n",
        "\n",
        "zip_path = os.path.join(base_drive_path, \"UCF-101.zip\")\n",
        "\n",
        "# Ø±Ø§Ø¨Ø· Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø¶ØºÙˆØ·Ø© (~6.5GB)\n",
        "hf_url = \"https://huggingface.co/datasets/quchenyuan/UCF101-ZIP/resolve/main/UCF-101.zip\"\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù (Ù„Ùˆ Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø¨Ù‚Ø§Ù‹ ÙÙŠ Ø§Ù„Ø¯Ø±Ø§ÙŠÙ)\n",
        "if not os.path.exists(zip_path):\n",
        "    print(\"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ UCF101 ZIP ...\")\n",
        "    !gdown {hf_url} -O {zip_path}\n",
        "\n",
        "# ========================================\n",
        "# STEP 2: ÙÙƒ Ø¶ØºØ· ZIP Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¯Ø±Ø§ÙŠÙ\n",
        "# ========================================\n",
        "dataset_folder = os.path.join(base_drive_path, \"UCF-101\")\n",
        "if not os.path.exists(dataset_folder):\n",
        "    print(\"ğŸ“‚ ÙÙƒ Ø¶ØºØ· UCF101 ...\")\n",
        "    !unzip -q {zip_path} -d {base_drive_path}\n",
        "\n",
        "print(f\"âœ… ØªÙ… ÙÙƒ Ø§Ù„Ø¶ØºØ·ØŒ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ: {dataset_folder}\")\n",
        "\n",
        "# ========================================\n",
        "# STEP 3: Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n",
        "# ========================================\n",
        "selected_classes = [\"BasketballDunk\", \"HorseRiding\", \"PlayingGuitar\", \"PushUps\"]\n",
        "subset_folder = os.path.join(base_drive_path, \"videos_subset\")\n",
        "os.makedirs(subset_folder, exist_ok=True)\n",
        "\n",
        "# Ù†Ø³Ø® Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ù…Ø®ØªØ§Ø±Ø© Ø¥Ù„Ù‰ Ù…Ø¬Ù„Ø¯ Ø¬Ø¯ÙŠØ¯\n",
        "for cls in selected_classes:\n",
        "    src_folder = os.path.join(dataset_folder, cls)\n",
        "    dst_folder = os.path.join(subset_folder, cls)\n",
        "    os.makedirs(dst_folder, exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(src_folder):\n",
        "        print(f\"âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ÙØ¦Ø©: {cls}\")\n",
        "        continue\n",
        "\n",
        "    for file in os.listdir(src_folder):\n",
        "        if file.endswith(\".avi\"):\n",
        "            shutil.copy2(os.path.join(src_folder, file), dst_folder)\n",
        "\n",
        "print(f\"âœ… ØªÙ… Ù†Ø³Ø® Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ù…Ø®ØªØ§Ø±Ø© Ø¥Ù„Ù‰: {subset_folder}\")\n",
        "\n",
        "# ========================================\n",
        "# STEP 4: Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ZIP Ù„Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©\n",
        "# ========================================\n",
        "zip_subset_path = os.path.join(base_drive_path, \"videos_subset.zip\")\n",
        "shutil.make_archive(zip_subset_path.replace(\".zip\", \"\"), 'zip', subset_folder)\n",
        "print(f\"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ZIP: {zip_subset_path}\")\n",
        "\n",
        "# ========================================\n",
        "# STEP 5: Ø­Ø³Ø§Ø¨ Ø­Ø¬Ù… Ø§Ù„Ù…Ø¬Ù„Ø¯\n",
        "# ========================================\n",
        "def get_folder_size(folder_path):\n",
        "    total_size = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
        "        for f in filenames:\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            total_size += os.path.getsize(fp)\n",
        "    return total_size\n",
        "\n",
        "size_bytes = get_folder_size(subset_folder)\n",
        "size_gb = size_bytes / (1024**3)\n",
        "\n",
        "print(f\"ğŸ’¾ Ø­Ø¬Ù… Ø§Ù„Ù…Ø¬Ù„Ø¯ videos_subset: {size_gb:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd3oZeO3NexC",
        "outputId": "3b4b86c5-d57c-4f98-9f0f-7d8ef0586268"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkYrPhj6NoIF"
      },
      "outputs": [],
      "source": [
        "subset_folder = \"/content/drive/MyDrive/UCF101_project/videos_subset\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bmV0y3kOCm7",
        "outputId": "39dbf1f7-5405-4882-d087-62c69bb4644c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Ø§Ù„Ù…Ø¬Ù„Ø¯ Ù…ÙˆØ¬ÙˆØ¯: /content/drive/MyDrive/UCF101_project/UCF-101\n",
            "Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯:\n",
            "- BreastStroke: 101 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- Fencing: 111 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- HighJump: 123 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- ApplyLipstick: 114 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- BodyWeightSquats: 112 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- PlayingPiano: 105 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- PommelHorse: 123 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- CliffDiving: 0 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- FieldHockeyPenalty: 111 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- SalsaSpin: 133 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- FloorGymnastics: 125 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- Skijet: 100 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- Haircut: 130 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- TrampolineJumping: 119 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- LongJump: 131 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- ParallelBars: 114 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- Rafting: 111 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- ShavingBeard: 161 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- SkyDiving: 110 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- SumoWrestling: 116 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- TableTennisShot: 0 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- Typing: 135 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- Nunchucks: 132 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- PushUps: 0 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- JugglingBalls: 121 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- Kayaking: 141 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- Skiing: 135 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- Swing: 131 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- HorseRace: 0 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- Archery: 143 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- PlayingFlute: 155 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- SoccerPenalty: 137 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- FrontCrawl: 137 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- Basketball: 134 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- Drumming: 161 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- YoYo: 128 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- PlayingViolin: 100 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- WalkingWithDog: 123 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- HorseRiding: 161 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- Surfing: 126 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- BoxingSpeedBag: 134 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- CuttingInKitchen: 110 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- HammerThrow: 150 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- RockClimbingIndoor: 144 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- JumpRope: 144 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- Rowing: 137 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- ThrowDiscus: 130 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- FrisbeeCatch: 126 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- PlayingDaf: 151 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- MilitaryParade: 125 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- PlayingGuitar: 158 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- SoccerJuggling: 147 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- TennisSwing: 166 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- Shotput: 144 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- Bowling: 155 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- StillRings: 112 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- HulaHoop: 125 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- SkateBoarding: 120 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- BasketballDunk: 131 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- WritingOnBoard: 152 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- BrushingTeeth: 131 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- IceDancing: 158 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n",
            "- videos_subset: 0 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø®ØªØ§Ø±\n",
        "subset_folder = \"/content/drive/MyDrive/UCF101_project/UCF-101\"\n",
        "\n",
        "# Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ø¬Ù„Ø¯\n",
        "if os.path.exists(subset_folder):\n",
        "    print(\"âœ… Ø§Ù„Ù…Ø¬Ù„Ø¯ Ù…ÙˆØ¬ÙˆØ¯:\", subset_folder)\n",
        "\n",
        "    # Ø¹Ø±Ø¶ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø¯Ø§Ø®Ù„Ù‡\n",
        "    classes = os.listdir(subset_folder)\n",
        "    print(\"Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯:\")\n",
        "    for cls in classes:\n",
        "        cls_path = os.path.join(subset_folder, cls)\n",
        "        # Ø¹Ø¯Ø¯ Ù…Ù„ÙØ§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù„ÙƒÙ„ ÙØ¦Ø©\n",
        "        videos = [f for f in os.listdir(cls_path) if f.endswith(\".avi\")]\n",
        "        print(f\"- {cls}: {len(videos)} ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\")\n",
        "else:\n",
        "    print(\"âš ï¸ Ø§Ù„Ù…Ø¬Ù„Ø¯ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bJQKd6fQD7g",
        "outputId": "84cfee1b-e779-417b-fc42-ea1472db7262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ØªÙ… Ù†Ø³Ø® Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø¥Ù„Ù‰: /content/drive/MyDrive/UCF101_project/UCF-101/videos_subset\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "base_folder = \"/content/drive/MyDrive/UCF101_project/UCF-101\"\n",
        "subset_folder = os.path.join(base_folder, \"videos_subset\")\n",
        "os.makedirs(subset_folder, exist_ok=True)\n",
        "\n",
        "classes = [\"BasketballDunk\", \"HorseRiding\", \"PlayingGuitar\", \"Archery\"]\n",
        "\n",
        "for cls in classes:\n",
        "    src_folder = os.path.join(base_folder, cls)\n",
        "    dst_folder = os.path.join(subset_folder, cls)\n",
        "    os.makedirs(dst_folder, exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(src_folder):\n",
        "        print(f\"âš ï¸ Ø§Ù„ÙØ¦Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©: {cls}\")\n",
        "        continue\n",
        "\n",
        "    for file in os.listdir(src_folder):\n",
        "        if file.endswith(\".avi\"):\n",
        "            shutil.copy2(os.path.join(src_folder, file), dst_folder)\n",
        "\n",
        "print(f\"âœ… ØªÙ… Ù†Ø³Ø® Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø¥Ù„Ù‰: {subset_folder}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhgIe9XQOg0D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToiV3ruodkll"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnA-3lHxXbtT",
        "outputId": "0cc6ba73-ece8-4fe1-a567-af3c3cfc4957"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-241129296.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;31m# ØªÙ‚ÙŠÙŠÙ… Ø¹Ù„Ù‰ validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# 1ï¸âƒ£ ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n",
        "# ========================================\n",
        "!pip install torch torchvision transformers accelerate datasets gradio opencv-python --quiet\n",
        "\n",
        "# ========================================\n",
        "# 2ï¸âƒ£ Ø±Ø¨Ø· Google Drive\n",
        "# ========================================\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ========================================\n",
        "# 3ï¸âƒ£ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª ÙˆØ§Ù„ÙØ¦Ø§Øª\n",
        "# ========================================\n",
        "dataset_folder = \"/content/drive/MyDrive/UCF101_project/UCF-101\"\n",
        "save_model_path = \"/content/drive/MyDrive/UCF101_project/blip_finetunneedd\"\n",
        "os.makedirs(save_model_path, exist_ok=True)\n",
        "\n",
        "classes = [\"ApplyLipstick\", \"PlayingPiano\", \"TrampolineJumping\", \"ShavingBeard\"]\n",
        "class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
        "\n",
        "# ========================================\n",
        "# 4ï¸âƒ£ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¥Ù„Ù‰ frames\n",
        "# ========================================\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def video_to_frames(video_path, max_frames=16):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    count = 0\n",
        "    while cap.isOpened() and count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "        count += 1\n",
        "    cap.release()\n",
        "    frames = np.array(frames)\n",
        "    return frames\n",
        "\n",
        "# ========================================\n",
        "# 5ï¸âƒ£ Dataset Ù…Ø¹ Augmentation\n",
        "# ========================================\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª (Augmentation + Normalization)\n",
        "transform = T.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomRotation(10),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "class VideoFramesDataset(Dataset):\n",
        "    def __init__(self, root_dir, classes, max_frames=16, transform=None):\n",
        "        self.samples = []\n",
        "        self.labels = []\n",
        "        self.classes = classes\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
        "        self.max_frames = max_frames\n",
        "        self.transform = transform\n",
        "\n",
        "        for cls in classes:\n",
        "            cls_folder = os.path.join(root_dir, cls)\n",
        "            if not os.path.exists(cls_folder):\n",
        "                print(f\"âš ï¸ Ø§Ù„ÙØ¦Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©: {cls}\")\n",
        "                continue\n",
        "            for file in os.listdir(cls_folder):\n",
        "                if file.endswith(\".avi\"):\n",
        "                    self.samples.append(os.path.join(cls_folder, file))\n",
        "                    self.labels.append(self.class_to_idx[cls])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.samples[idx]\n",
        "        frames = video_to_frames(video_path, self.max_frames)\n",
        "        processed_frames = []\n",
        "\n",
        "        for f in frames:\n",
        "            if self.transform:\n",
        "                f = self.transform(f)\n",
        "            processed_frames.append(f)\n",
        "        frames = torch.stack(processed_frames)  # (T, C, H, W)\n",
        "        label = self.labels[idx]\n",
        "        return frames, label\n",
        "\n",
        "# ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "dataset = VideoFramesDataset(dataset_folder, classes, max_frames=16, transform=transform)\n",
        "train_idx, test_idx = train_test_split(range(len(dataset)), test_size=0.2, stratify=dataset.labels)\n",
        "train_idx, val_idx = train_test_split(train_idx, test_size=0.1, stratify=[dataset.labels[i] for i in train_idx])\n",
        "\n",
        "train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
        "val_dataset = torch.utils.data.Subset(dataset, val_idx)\n",
        "test_dataset = torch.utils.data.Subset(dataset, test_idx)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, num_workers=2)\n",
        "\n",
        "# ========================================\n",
        "# 6ï¸âƒ£ Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆØ°Ø¬ BLIP + Classifier (Fine-tuning Ù…Ø­Ø³Ù‘Ù†)\n",
        "# ========================================\n",
        "from transformers import BlipForConditionalGeneration, BlipProcessor\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "model_name = \"Salesforce/blip-image-captioning-base\"\n",
        "processor = BlipProcessor.from_pretrained(model_name)\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(model_name)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "blip_model.to(device)\n",
        "\n",
        "# ØªØ¬Ù…ÙŠØ¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø£ÙˆÙ„Ø§Ù‹\n",
        "for param in blip_model.vision_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "def unfreeze_last_layer():\n",
        "    for param in blip_model.vision_model.encoder.layers[-1].parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "# Ø¥Ù†Ø´Ø§Ø¡ classifier Ù…Ø¹ Dropout\n",
        "dummy_frame = torch.randn(1, 3, 224, 224).to(device)\n",
        "with torch.no_grad():\n",
        "    dummy_output = blip_model.vision_model(dummy_frame).last_hidden_state.mean(dim=1)\n",
        "hidden_dim = dummy_output.size(1)\n",
        "\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(hidden_dim, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(256, len(classes))\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "# ========================================\n",
        "# 7ï¸âƒ£ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Early Stopping ÙˆØ¹ÙƒØ³ Ø§Ù„ØªØ·Ø¨ÙŠØ¹\n",
        "# ========================================\n",
        "epochs = 3\n",
        "best_val_loss = float(\"inf\")\n",
        "patience = 2\n",
        "trigger_times = 0\n",
        "unfrozen = False\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    blip_model.train()\n",
        "    classifier.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for frames, labels in train_loader:\n",
        "        batch_size, T, C, H, W = frames.shape\n",
        "        frames = frames.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        video_embeds = []\n",
        "        for t in range(T):\n",
        "            # ğŸ” Ø¹ÙƒØ³ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ù‚Ø¨Ù„ ØªÙ…Ø±ÙŠØ± Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ BLIP\n",
        "            img = frames[:, t]\n",
        "            img = img * torch.tensor([0.229, 0.224, 0.225], device=device).view(1,3,1,1)\n",
        "            img = img + torch.tensor([0.485, 0.456, 0.406], device=device).view(1,3,1,1)\n",
        "            img = img.clamp(0,1)\n",
        "\n",
        "            pixel_values = processor(\n",
        "                images=img.permute(0, 2, 3, 1).cpu().numpy(),\n",
        "                return_tensors=\"pt\",\n",
        "                do_rescale=False\n",
        "            ).pixel_values.to(device)\n",
        "\n",
        "            output = blip_model.vision_model(pixel_values).last_hidden_state.mean(dim=1)\n",
        "            video_embeds.append(output)\n",
        "        video_embeds = torch.stack(video_embeds).mean(dim=0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = classifier(video_embeds)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # ØªÙ‚ÙŠÙŠÙ… Ø¹Ù„Ù‰ validation\n",
        "    blip_model.eval()\n",
        "    classifier.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for frames, labels in val_loader:\n",
        "            frames = frames.to(device)\n",
        "            labels = labels.to(device)\n",
        "            video_embeds = []\n",
        "            for t in range(frames.shape[1]):\n",
        "                img = frames[:, t]\n",
        "                img = img * torch.tensor([0.229, 0.224, 0.225], device=device).view(1,3,1,1)\n",
        "                img = img + torch.tensor([0.485, 0.456, 0.406], device=device).view(1,3,1,1)\n",
        "                img = img.clamp(0,1)\n",
        "\n",
        "                pixel_values = processor(\n",
        "                    images=img.permute(0, 2, 3, 1).cpu().numpy(),\n",
        "                    return_tensors=\"pt\",\n",
        "                    do_rescale=False\n",
        "                ).pixel_values.to(device)\n",
        "\n",
        "                output = blip_model.vision_model(pixel_values).last_hidden_state.mean(dim=1)\n",
        "                video_embeds.append(output)\n",
        "            video_embeds = torch.stack(video_embeds).mean(dim=0)\n",
        "\n",
        "            logits = classifier(video_embeds)\n",
        "            val_loss += criterion(logits, labels).item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        trigger_times = 0\n",
        "        torch.save(classifier.state_dict(), os.path.join(save_model_path, \"best_classifier.pth\"))\n",
        "    else:\n",
        "        trigger_times += 1\n",
        "        print(f\"âš ï¸ EarlyStopping Trigger: {trigger_times}/{patience}\")\n",
        "        if trigger_times >= patience:\n",
        "            print(\"â›”ï¸ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø¨ÙƒØ± Ù„ØªØ¬Ù†Ù‘Ø¨ Overfitting\")\n",
        "            break\n",
        "\n",
        "    if epoch == 1 and not unfrozen:\n",
        "        print(\"ğŸ”“ ØªÙ… ÙÙƒ Ø¢Ø®Ø± Ø·Ø¨Ù‚Ø© Ù…Ù† BLIP Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ\")\n",
        "        unfreeze_last_layer()\n",
        "        optimizer = optim.Adam(\n",
        "            list(classifier.parameters()) + list(blip_model.vision_model.encoder.layers[-1].parameters()),\n",
        "            lr=1e-4, weight_decay=1e-5\n",
        "        )\n",
        "        unfrozen = True\n",
        "\n",
        "# ========================================\n",
        "# 8ï¸âƒ£ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¹Ù„Ù‰ test set\n",
        "# ========================================\n",
        "blip_model.eval()\n",
        "classifier.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for frames, labels in test_loader:\n",
        "        frames = frames.to(device)\n",
        "        labels = labels.to(device)\n",
        "        video_embeds = []\n",
        "        for t in range(frames.shape[1]):\n",
        "            img = frames[:, t]\n",
        "            img = img * torch.tensor([0.229, 0.224, 0.225], device=device).view(1,3,1,1)\n",
        "            img = img + torch.tensor([0.485, 0.456, 0.406], device=device).view(1,3,1,1)\n",
        "            img = img.clamp(0,1)\n",
        "\n",
        "            pixel_values = processor(\n",
        "                images=img.permute(0, 2, 3, 1).cpu().numpy(),\n",
        "                return_tensors=\"pt\",\n",
        "                do_rescale=False\n",
        "            ).pixel_values.to(device)\n",
        "\n",
        "            output = blip_model.vision_model(pixel_values).last_hidden_state.mean(dim=1)\n",
        "            video_embeds.append(output)\n",
        "        video_embeds = torch.stack(video_embeds).mean(dim=0)\n",
        "\n",
        "        logits = classifier(video_embeds)\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"âœ… Test Accuracy: {correct/total*100:.2f}%\")\n",
        "\n",
        "# ========================================\n",
        "# 9ï¸âƒ£ Ø­ÙØ¸ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¹Ù„Ù‰ Drive\n",
        "# ========================================\n",
        "blip_model.save_pretrained(save_model_path)\n",
        "torch.save(classifier.state_dict(), os.path.join(save_model_path, \"final_classifier.pth\"))\n",
        "print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙÙŠ: {save_model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YOq5ryLdrbbt"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ğŸ”Ÿ ÙˆØ§Ø¬Ù‡Ø© Gradio Ù„Ù„ØªØ¬Ø±Ø¨Ø©\n",
        "# ========================================\n",
        "import gradio as gr\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙˆØ§Ù„Ù€ classifier\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_name = \"Salesforce/blip-image-captioning-base\"\n",
        "processor = BlipProcessor.from_pretrained(model_name)\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(save_model_path).to(device)\n",
        "\n",
        "hidden_dim = blip_model.vision_model(torch.randn(1, 3, 224, 224).to(device)).last_hidden_state.mean(dim=1).size(1)\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(hidden_dim, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(256, len(classes))\n",
        ").to(device)\n",
        "\n",
        "classifier.load_state_dict(torch.load(os.path.join(save_model_path, \"final_classifier.pth\"), map_location=device))\n",
        "classifier.eval()\n",
        "blip_model.eval()\n",
        "\n",
        "# -------------------------------\n",
        "# Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¥Ù„Ù‰ Ø¥Ø·Ø§Ø±Ø§Øª\n",
        "# -------------------------------\n",
        "def extract_frames(video_path, max_frames=16):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    count = 0\n",
        "    while cap.isOpened() and count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "        count += 1\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "# -------------------------------\n",
        "# Ø¯Ø§Ù„Ø© Ø§Ù„ØªØµÙ†ÙŠÙ\n",
        "# -------------------------------\n",
        "def classify_video(video):\n",
        "    frames = extract_frames(video)\n",
        "    if len(frames) == 0:\n",
        "        return \"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø±Ø§Øª ÙÙŠ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ\", None, None\n",
        "\n",
        "    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø¥Ù„Ù‰ Tensor\n",
        "    frames = [transform(f) for f in frames]\n",
        "    frames = torch.stack(frames).unsqueeze(0).to(device)  # (1, T, C, H, W)\n",
        "\n",
        "    T = frames.shape[1]\n",
        "    video_embeds = []\n",
        "    with torch.no_grad():\n",
        "        for t in range(T):\n",
        "            img = frames[:, t]\n",
        "            img = img * torch.tensor([0.229, 0.224, 0.225], device=device).view(1,3,1,1)\n",
        "            img = img + torch.tensor([0.485, 0.456, 0.406], device=device).view(1,3,1,1)\n",
        "            img = img.clamp(0,1)\n",
        "\n",
        "            pixel_values = processor(\n",
        "                images=img.permute(0, 2, 3, 1).cpu().numpy(),\n",
        "                return_tensors=\"pt\",\n",
        "                do_rescale=False\n",
        "            ).pixel_values.to(device)\n",
        "\n",
        "            output = blip_model.vision_model(pixel_values).last_hidden_state.mean(dim=1)\n",
        "            video_embeds.append(output)\n",
        "        video_embeds = torch.stack(video_embeds).mean(dim=0)\n",
        "\n",
        "        logits = classifier(video_embeds)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        conf, pred = torch.max(probs, 1)\n",
        "\n",
        "    predicted_class = classes[pred.item()]\n",
        "    confidence = conf.item() * 100\n",
        "\n",
        "    # Ø´Ø±Ø­ Ù†ØµÙŠ Ø¨Ø³ÙŠØ· Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø©\n",
        "    explanations = {\n",
        "        \"ApplyLipstick\": \"Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠÙØ¸Ù‡Ø± Ø´Ø®ØµÙ‹Ø§ ÙŠÙ‚ÙˆÙ… Ø¨ÙˆØ¶Ø¹ Ø£Ø­Ù…Ø± Ø´ÙØ§Ù‡.\",\n",
        "        \"PlayingPiano\": \"Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠÙØ¸Ù‡Ø± Ø´Ø®ØµÙ‹Ø§ ÙŠØ¹Ø²Ù Ø¹Ù„Ù‰ Ø¢Ù„Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ùˆ.\",\n",
        "        \"TrampolineJumping\": \"Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø´Ø®Øµ ÙŠÙ‚ÙØ² Ø¹Ù„Ù‰ Ø§Ù„ØªØ±Ø§Ù…Ø¨ÙˆÙ„ÙŠÙ†.\",\n",
        "        \"ShavingBeard\": \"Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠÙØ¸Ù‡Ø± Ø¹Ù…Ù„ÙŠØ© Ø­Ù„Ø§Ù‚Ø© Ø§Ù„Ù„Ø­ÙŠØ©.\"\n",
        "    }\n",
        "\n",
        "    return f\"ğŸ¬ Ø§Ù„ØªØµÙ†ÙŠÙ: {predicted_class}\", explanations[predicted_class], f\"ğŸ”¹ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©: {confidence:.2f} / 100\"\n",
        "\n",
        "# -------------------------------\n",
        "# Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø© Gradio\n",
        "# -------------------------------\n",
        "interface = gr.Interface(\n",
        "    fn=classify_video,\n",
        "    inputs=gr.Video(label=\"ğŸ“¹ Ø§Ø±ÙØ¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù‡Ù†Ø§\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"ğŸ” Ø§Ù„Ù†ØªÙŠØ¬Ø©\"),\n",
        "        gr.Textbox(label=\"ğŸ“ Ø§Ù„Ø´Ø±Ø­\"),\n",
        "        gr.Textbox(label=\"ğŸ“Š Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©\")\n",
        "    ],\n",
        "    title=\"ğŸ¥ ØªØµÙ†ÙŠÙ Ø£Ù†Ø´Ø·Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BLIP\",\n",
        "    description=\"Ø§Ø±ÙØ¹ ÙÙŠØ¯ÙŠÙˆ Ù‚ØµÙŠØ± (â‰¤10 Ø«ÙˆØ§Ù†ÙŠ). Ø³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª ÙˆØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ø´Ø§Ø· Ø¯Ø§Ø®Ù„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©.\"\n",
        ")\n",
        "\n",
        "interface.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "6SOwyPCtXkEP",
        "outputId": "e0f3f762-aa7b-4947-e707-c063aa1f4a77"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Trying to override a python impl for DispatchKey.Meta on operator aten::broadcast_tensors",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2860732701.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ========================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2667\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2668\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m \u001b[0;31m# Enable CUDA Sanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSymBool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSymFloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m from torch._decomp import (\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0m_add_op_to_registry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0m_convert_out_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;31m# populate the table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompositions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_refs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2729\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDispatchKey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompositeImplicitAutograd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2730\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDispatchKey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2731\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensorLikeType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2732\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_kernels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m    153\u001b[0m                     \u001b[0;34mf\"Trying to override a python impl for {k} on operator {self.name()}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to override a python impl for DispatchKey.Meta on operator aten::broadcast_tensors"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# ğŸ”Ÿ ÙˆØ§Ø¬Ù‡Ø© Gradio Ù„Ù„ØªØ¬Ø±Ø¨Ø©\n",
        "# ========================================\n",
        "import gradio as gr\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙˆØ§Ù„Ù€ classifier\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_name = \"Salesforce/blip-image-captioning-base\"\n",
        "processor = BlipProcessor.from_pretrained(model_name)\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(save_model_path).to(device)\n",
        "\n",
        "hidden_dim = blip_model.vision_model(torch.randn(1, 3, 224, 224).to(device)).last_hidden_state.mean(dim=1).size(1)\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(hidden_dim, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(256, len(classes))\n",
        ").to(device)\n",
        "\n",
        "classifier.load_state_dict(torch.load(os.path.join(save_model_path, \"final_classifier.pth\"), map_location=device))\n",
        "classifier.eval()\n",
        "blip_model.eval()\n",
        "\n",
        "# -------------------------------\n",
        "# Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¥Ù„Ù‰ Ø¥Ø·Ø§Ø±Ø§Øª\n",
        "# -------------------------------\n",
        "def extract_frames(video_path, max_frames=16):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    count = 0\n",
        "    while cap.isOpened() and count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "        count += 1\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "# -------------------------------\n",
        "# Ø¯Ø§Ù„Ø© Ø§Ù„ØªØµÙ†ÙŠÙ\n",
        "# -------------------------------\n",
        "def classify_video(video):\n",
        "    frames = extract_frames(video)\n",
        "    if len(frames) == 0:\n",
        "        return \"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø±Ø§Øª ÙÙŠ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ\", None, None\n",
        "\n",
        "    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø¥Ù„Ù‰ Tensor\n",
        "    frames = [transform(f) for f in frames]\n",
        "    frames = torch.stack(frames).unsqueeze(0).to(device)  # (1, T, C, H, W)\n",
        "\n",
        "    T = frames.shape[1]\n",
        "    video_embeds = []\n",
        "    with torch.no_grad():\n",
        "        for t in range(T):\n",
        "            img = frames[:, t]\n",
        "            img = img * torch.tensor([0.229, 0.224, 0.225], device=device).view(1,3,1,1)\n",
        "            img = img + torch.tensor([0.485, 0.456, 0.406], device=device).view(1,3,1,1)\n",
        "            img = img.clamp(0,1)\n",
        "\n",
        "            pixel_values = processor(\n",
        "                images=img.permute(0, 2, 3, 1).cpu().numpy(),\n",
        "                return_tensors=\"pt\",\n",
        "                do_rescale=False\n",
        "            ).pixel_values.to(device)\n",
        "\n",
        "            output = blip_model.vision_model(pixel_values).last_hidden_state.mean(dim=1)\n",
        "            video_embeds.append(output)\n",
        "        video_embeds = torch.stack(video_embeds).mean(dim=0)\n",
        "\n",
        "        logits = classifier(video_embeds)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        conf, pred = torch.max(probs, 1)\n",
        "\n",
        "    predicted_class = classes[pred.item()]\n",
        "    confidence = conf.item() * 100\n",
        "\n",
        "    # Ø´Ø±Ø­ Ù†ØµÙŠ Ø¨Ø³ÙŠØ· Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø©\n",
        "    explanations = {\n",
        "        \"ApplyLipstick\": \"Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠÙØ¸Ù‡Ø± Ø´Ø®ØµÙ‹Ø§ ÙŠÙ‚ÙˆÙ… Ø¨ÙˆØ¶Ø¹ Ø£Ø­Ù…Ø± Ø´ÙØ§Ù‡.\",\n",
        "        \"PlayingPiano\": \"Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠÙØ¸Ù‡Ø± Ø´Ø®ØµÙ‹Ø§ ÙŠØ¹Ø²Ù Ø¹Ù„Ù‰ Ø¢Ù„Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ùˆ.\",\n",
        "        \"TrampolineJumping\": \"Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø´Ø®Øµ ÙŠÙ‚ÙØ² Ø¹Ù„Ù‰ Ø§Ù„ØªØ±Ø§Ù…Ø¨ÙˆÙ„ÙŠÙ†.\",\n",
        "        \"ShavingBeard\": \"Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠÙØ¸Ù‡Ø± Ø¹Ù…Ù„ÙŠØ© Ø­Ù„Ø§Ù‚Ø© Ø§Ù„Ù„Ø­ÙŠØ©.\"\n",
        "    }\n",
        "\n",
        "    return f\"ğŸ¬ Ø§Ù„ØªØµÙ†ÙŠÙ: {predicted_class}\", explanations[predicted_class], f\"ğŸ”¹ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©: {confidence:.2f} / 100\"\n",
        "\n",
        "# -------------------------------\n",
        "# Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø© Gradio\n",
        "# -------------------------------\n",
        "interface = gr.Interface(\n",
        "    fn=classify_video,\n",
        "    inputs=gr.Video(label=\"ğŸ“¹ Ø§Ø±ÙØ¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù‡Ù†Ø§\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"ğŸ” Ø§Ù„Ù†ØªÙŠØ¬Ø©\"),\n",
        "        gr.Textbox(label=\"ğŸ“ Ø§Ù„Ø´Ø±Ø­\"),\n",
        "        gr.Textbox(label=\"ğŸ“Š Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©\")\n",
        "    ],\n",
        "    title=\"ğŸ¥ ØªØµÙ†ÙŠÙ Ø£Ù†Ø´Ø·Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BLIP\",\n",
        "    description=\"Ø§Ø±ÙØ¹ ÙÙŠØ¯ÙŠÙˆ Ù‚ØµÙŠØ± (â‰¤10 Ø«ÙˆØ§Ù†ÙŠ). Ø³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª ÙˆØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ø´Ø§Ø· Ø¯Ø§Ø®Ù„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©.\"\n",
        ")\n",
        "\n",
        "interface.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBXZabUpXEoi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IsaDxztgGkw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-QaoN6vgdZH",
        "outputId": "f05c3277-5cdb-4a03-808b-1553cbb804e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "classifier.pth\tconfig.json  generation_config.json\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/UCF101_project/blip_finetuned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Kl_HFwCamEki",
        "outputId": "902b5a89-4a61-4529-c84d-02b8e7b03a95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Collecting gradio==4.31.0\n",
            "  Downloading gradio-4.31.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==4.31.0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.31.0) (5.5.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from gradio==4.31.0) (0.120.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio==4.31.0) (0.6.4)\n",
            "Collecting gradio-client==0.16.2 (from gradio==4.31.0)\n",
            "  Downloading gradio_client-0.16.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio==4.31.0) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.12/dist-packages (from gradio==4.31.0) (0.36.0)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.12/dist-packages (from gradio==4.31.0) (6.5.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.31.0) (3.1.6)\n",
            "Collecting markupsafe~=2.0 (from gradio==4.31.0)\n",
            "  Downloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.31.0) (3.10.0)\n",
            "Collecting numpy~=1.0 (from gradio==4.31.0)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.31.0) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio==4.31.0) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.31.0) (2.2.2)\n",
            "Collecting pillow<11.0,>=8.0 (from gradio==4.31.0)\n",
            "  Downloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.31.0) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio==4.31.0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from gradio==4.31.0) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.31.0) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from gradio==4.31.0) (0.14.2)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.31.0) (2.10.0)\n",
            "Collecting tomlkit==0.12.0 (from gradio==4.31.0)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio==4.31.0) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.31.0) (4.15.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.31.0) (2.5.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.31.0) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==0.16.2->gradio==4.31.0) (2025.3.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.16.2->gradio==4.31.0)\n",
            "  Downloading websockets-11.0.3-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair<6.0,>=4.2.0->gradio==4.31.0) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair<6.0,>=4.2.0->gradio==4.31.0) (2.9.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==4.31.0) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==4.31.0) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==4.31.0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==4.31.0) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio==4.31.0) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.3->gradio==4.31.0) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.31.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.31.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.31.0) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.31.0) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.31.0) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.31.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio==4.31.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio==4.31.0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->gradio==4.31.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->gradio==4.31.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->gradio==4.31.0) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio==4.31.0) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio==4.31.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio==4.31.0) (13.9.4)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi->gradio==4.31.0) (0.48.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi->gradio==4.31.0) (0.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.31.0) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.31.0) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.31.0) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.31.0) (0.28.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==4.31.0) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.31.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.31.0) (2.19.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.24.1->gradio==4.31.0) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio==4.31.0) (0.1.2)\n",
            "Downloading gradio-4.31.0-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m122.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-0.16.2-py3-none-any.whl (315 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m315.5/315.5 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-11.0.3-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: websockets, tomlkit, pillow, numpy, markupsafe, aiofiles, opencv-python, gradio-client, gradio\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: tomlkit\n",
            "    Found existing installation: tomlkit 0.13.3\n",
            "    Uninstalling tomlkit-0.13.3:\n",
            "      Successfully uninstalled tomlkit-0.13.3\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.3\n",
            "    Uninstalling MarkupSafe-3.0.3:\n",
            "      Successfully uninstalled MarkupSafe-3.0.3\n",
            "  Attempting uninstall: aiofiles\n",
            "    Found existing installation: aiofiles 24.1.0\n",
            "    Uninstalling aiofiles-24.1.0:\n",
            "      Successfully uninstalled aiofiles-24.1.0\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.12.0.88\n",
            "    Uninstalling opencv-python-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-4.12.0.88\n",
            "  Attempting uninstall: gradio-client\n",
            "    Found existing installation: gradio_client 1.13.3\n",
            "    Uninstalling gradio_client-1.13.3:\n",
            "      Successfully uninstalled gradio_client-1.13.3\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 5.49.1\n",
            "    Uninstalling gradio-5.49.1:\n",
            "      Successfully uninstalled gradio-5.49.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.46.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 11.0.3 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "yfinance 0.2.66 requires websockets>=13.0, but you have websockets 11.0.3 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "google-adk 1.17.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 11.0.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires websockets>=14.0, but you have websockets 11.0.3 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 gradio-4.31.0 gradio-client-0.16.2 markupsafe-2.1.5 numpy-1.26.4 opencv-python-4.11.0.86 pillow-10.4.0 tomlkit-0.12.0 websockets-11.0.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "f0d54604ee1240b397322ac732020956",
              "pip_warning": {
                "packages": [
                  "PIL",
                  "aiofiles",
                  "cv2",
                  "gradio",
                  "numpy",
                  "tomlkit",
                  "websockets"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-526719525.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# ğŸš€ ØªØ­Ù…ÙŠÙ„ Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# ----------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# ----------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# ğŸ”§ ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n",
        "# ========================================\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install transformers gradio==4.31.0 opencv-python\n",
        "\n",
        "# ========================================\n",
        "# ğŸ”Ÿ ÙˆØ§Ø¬Ù‡Ø© Gradio Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BLIP\n",
        "# ========================================\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import cv2\n",
        "import gradio as gr\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from google.colab import drive\n",
        "\n",
        "# ----------------------------------------\n",
        "# ğŸš€ ØªØ­Ù…ÙŠÙ„ Google Drive\n",
        "# ----------------------------------------\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ----------------------------------------\n",
        "# ğŸ”§ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¬Ù‡Ø§Ø² ÙˆØ§Ù„Ù…ÙˆØ¯ÙŠÙ„\n",
        "# ----------------------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"ğŸ–¥ï¸ Using device:\", device)\n",
        "\n",
        "# Ù…Ø³Ø§Ø± Ø§Ù„Ø­ÙØ¸ Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø®ØµØµ (Ø¹Ø¯Ù„Ù‡ Ø­Ø³Ø¨ Ù…ÙƒØ§Ù†Ùƒ)\n",
        "save_model_path = \"/content/drive/MyDrive/BLIP_model\"\n",
        "\n",
        "# ÙÙŠ Ø­Ø§Ù„ Ù„Ù… ÙŠÙƒÙ† Ù„Ø¯ÙŠÙƒ Ù…ÙˆØ¯ÙŠÙ„ Ù…Ø®ØµØµØŒ Ø³ÙŠÙØ­Ù…Ù‘Ù„ Ù…ÙˆØ¯ÙŠÙ„ BLIP Ø§Ù„Ø£ØµÙ„ÙŠ\n",
        "if not os.path.exists(save_model_path):\n",
        "    os.makedirs(save_model_path, exist_ok=True)\n",
        "    model_name = \"Salesforce/blip-image-captioning-base\"\n",
        "else:\n",
        "    model_name = save_model_path\n",
        "\n",
        "# ----------------------------------------\n",
        "# ğŸ§  ØªØ­Ù…ÙŠÙ„ BLIP Processor ÙˆØ§Ù„Ù…ÙˆØ¯ÙŠÙ„\n",
        "# ----------------------------------------\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "\n",
        "# ----------------------------------------\n",
        "# ğŸ”¢ ØªØ¹Ø±ÙŠÙ Ø§Ù„ÙØ¦Ø§Øª\n",
        "# ----------------------------------------\n",
        "classes = [\"ApplyLipstick\", \"PlayingPiano\", \"TrampolineJumping\", \"ShavingBeard\"]\n",
        "\n",
        "# ----------------------------------------\n",
        "# ğŸ§© Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù€ Classifier\n",
        "# ----------------------------------------\n",
        "with torch.no_grad():\n",
        "    dummy = torch.randn(1, 3, 224, 224).to(device)\n",
        "    hidden_dim = blip_model.vision_model(dummy).last_hidden_state.mean(dim=1).size(1)\n",
        "\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(hidden_dim, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(256, len(classes))\n",
        ").to(device)\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…ØµÙ†Ù Ø¥Ù† ÙˆØ¬Ø¯Øª\n",
        "clf_path = os.path.join(save_model_path, \"final_classifier.pth\")\n",
        "if os.path.exists(clf_path):\n",
        "    classifier.load_state_dict(torch.load(clf_path, map_location=device))\n",
        "    print(\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…ØµÙ†Ù.\")\n",
        "else:\n",
        "    print(\"âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…ØµÙ†ÙØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ØµÙ†Ù Ù…Ø¨Ø¯Ø¦ÙŠ.\")\n",
        "\n",
        "classifier.eval()\n",
        "blip_model.eval()\n",
        "\n",
        "# ----------------------------------------\n",
        "# ğŸ§° Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©\n",
        "# ----------------------------------------\n",
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def extract_frames(video_path, max_frames=16):\n",
        "    \"\"\"ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    count = 0\n",
        "    while cap.isOpened() and count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "        count += 1\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "# ----------------------------------------\n",
        "# ğŸ§® Ø¯Ø§Ù„Ø© Ø§Ù„ØªØµÙ†ÙŠÙ\n",
        "# ----------------------------------------\n",
        "def classify_video(video):\n",
        "    frames = extract_frames(video)\n",
        "    if len(frames) == 0:\n",
        "        return \"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø±Ø§Øª ÙÙŠ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ\", None, None\n",
        "\n",
        "    frames = [transform(f) for f in frames]\n",
        "    frames = torch.stack(frames).unsqueeze(0).to(device)  # (1, T, C, H, W)\n",
        "    T = frames.shape[1]\n",
        "\n",
        "    video_embeds = []\n",
        "    with torch.no_grad():\n",
        "        for t in range(T):\n",
        "            img = frames[:, t]\n",
        "            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„ØµÙˆØ± ÙˆÙÙ‚ BLIP\n",
        "            img = img * torch.tensor([0.229, 0.224, 0.225], device=device).view(1,3,1,1)\n",
        "            img = img + torch.tensor([0.485, 0.456, 0.406], device=device).view(1,3,1,1)\n",
        "            img = img.clamp(0,1)\n",
        "\n",
        "            pixel_values = processor(\n",
        "                images=img.permute(0, 2, 3, 1).cpu().numpy(),\n",
        "                return_tensors=\"pt\",\n",
        "                do_rescale=False\n",
        "            ).pixel_values.to(device)\n",
        "\n",
        "            output = blip_model.vision_model(pixel_values).last_hidden_state.mean(dim=1)\n",
        "            video_embeds.append(output)\n",
        "\n",
        "        video_embeds = torch.stack(video_embeds).mean(dim=0)\n",
        "        logits = classifier(video_embeds)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        conf, pred = torch.max(probs, 1)\n",
        "\n",
        "    predicted_class = classes[pred.item()]\n",
        "    confidence = conf.item() * 100\n",
        "\n",
        "    explanations = {\n",
        "        \"ApplyLipstick\": \"Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠÙØ¸Ù‡Ø± Ø´Ø®ØµÙ‹Ø§ ÙŠÙ‚ÙˆÙ… Ø¨ÙˆØ¶Ø¹ Ø£Ø­Ù…Ø± Ø´ÙØ§Ù‡.\",\n",
        "        \"PlayingPiano\": \"Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠÙØ¸Ù‡Ø± Ø´Ø®ØµÙ‹Ø§ ÙŠØ¹Ø²Ù Ø¹Ù„Ù‰ Ø¢Ù„Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ùˆ.\",\n",
        "        \"TrampolineJumping\": \"Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø´Ø®Øµ ÙŠÙ‚ÙØ² Ø¹Ù„Ù‰ Ø§Ù„ØªØ±Ø§Ù…Ø¨ÙˆÙ„ÙŠÙ†.\",\n",
        "        \"ShavingBeard\": \"Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠÙØ¸Ù‡Ø± Ø¹Ù…Ù„ÙŠØ© Ø­Ù„Ø§Ù‚Ø© Ø§Ù„Ù„Ø­ÙŠØ©.\"\n",
        "    }\n",
        "\n",
        "    return f\"ğŸ¬ Ø§Ù„ØªØµÙ†ÙŠÙ: {predicted_class}\", explanations[predicted_class], f\"ğŸ“Š Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©: {confidence:.2f}%\"\n",
        "\n",
        "# ----------------------------------------\n",
        "# ğŸ–¼ï¸ ÙˆØ§Ø¬Ù‡Ø© Gradio\n",
        "# ----------------------------------------\n",
        "interface = gr.Interface(\n",
        "    fn=classify_video,\n",
        "    inputs=gr.Video(label=\"ğŸ“¹ Ø§Ø±ÙØ¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù‡Ù†Ø§\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"ğŸ” Ø§Ù„Ù†ØªÙŠØ¬Ø©\"),\n",
        "        gr.Textbox(label=\"ğŸ“ Ø§Ù„Ø´Ø±Ø­\"),\n",
        "        gr.Textbox(label=\"ğŸ“Š Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©\")\n",
        "    ],\n",
        "    title=\"ğŸ¥ ØªØµÙ†ÙŠÙ Ø£Ù†Ø´Ø·Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BLIP\",\n",
        "    description=\"Ø§Ø±ÙØ¹ ÙÙŠØ¯ÙŠÙˆ Ù‚ØµÙŠØ± (â‰¤10 Ø«ÙˆØ§Ù†ÙŠ). Ø³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª ÙˆØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ø´Ø§Ø· Ø¯Ø§Ø®Ù„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©.\"\n",
        ")\n",
        "\n",
        "interface.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jrWXlzYTfbXu"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ğŸ”Ÿ ÙˆØ§Ø¬Ù‡Ø© Gradio Ù„Ù„ØªØ¬Ø±Ø¨Ø©\n",
        "# ========================================\n",
        "import gradio as gr\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙˆØ§Ù„Ù€ classifier\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_name = \"Salesforce/blip-image-captioning-base\"\n",
        "processor = BlipProcessor.from_pretrained(model_name)\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(save_model_path).to(device)\n",
        "\n",
        "hidden_dim = blip_model.vision_model(torch.randn(1, 3, 224, 224).to(device)).last_hidden_state.mean(dim=1).size(1)\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(hidden_dim, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(256, len(classes))\n",
        ").to(device)\n",
        "\n",
        "classifier.load_state_dict(torch.load(os.path.join(save_model_path, \"final_classifier.pth\"), map_location=device))\n",
        "classifier.eval()\n",
        "blip_model.eval()\n",
        "\n",
        "# -------------------------------\n",
        "# Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¥Ù„Ù‰ Ø¥Ø·Ø§Ø±Ø§Øª\n",
        "# -------------------------------\n",
        "def extract_frames(video_path, max_frames=16):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    count = 0\n",
        "    while cap.isOpened() and count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "        count += 1\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "# -------------------------------\n",
        "# Ø¯Ø§Ù„Ø© Ø§Ù„ØªØµÙ†ÙŠÙ\n",
        "# -------------------------------\n",
        "def classify_video(video):\n",
        "    frames = extract_frames(video)\n",
        "    if len(frames) == 0:\n",
        "        return \"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø±Ø§Øª ÙÙŠ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ\", None, None\n",
        "\n",
        "    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø¥Ù„Ù‰ Tensor\n",
        "    frames = [transform(f) for f in frames]\n",
        "    frames = torch.stack(frames).unsqueeze(0).to(device)  # (1, T, C, H, W)\n",
        "\n",
        "    T = frames.shape[1]\n",
        "    video_embeds = []\n",
        "    with torch.no_grad():\n",
        "        for t in range(T):\n",
        "            img = frames[:, t]\n",
        "            img = img * torch.tensor([0.229, 0.224, 0.225], device=device).view(1,3,1,1)\n",
        "            img = img + torch.tensor([0.485, 0.456, 0.406], device=device).view(1,3,1,1)\n",
        "            img = img.clamp(0,1)\n",
        "\n",
        "            pixel_values = processor(\n",
        "                images=img.permute(0, 2, 3, 1).cpu().numpy(),\n",
        "                return_tensors=\"pt\",\n",
        "                do_rescale=False\n",
        "            ).pixel_values.to(device)\n",
        "\n",
        "            output = blip_model.vision_model(pixel_values).last_hidden_state.mean(dim=1)\n",
        "            video_embeds.append(output)\n",
        "        video_embeds = torch.stack(video_embeds).mean(dim=0)\n",
        "\n",
        "        logits = classifier(video_embeds)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        conf, pred = torch.max(probs, 1)\n",
        "\n",
        "    predicted_class = classes[pred.item()]\n",
        "    confidence = conf.item() * 100\n",
        "\n",
        "    # Ø´Ø±Ø­ Ù†ØµÙŠ Ø¨Ø³ÙŠØ· Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø©\n",
        "    explanations = {\n",
        "        \"ApplyLipstick\": \"Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠÙØ¸Ù‡Ø± Ø´Ø®ØµÙ‹Ø§ ÙŠÙ‚ÙˆÙ… Ø¨ÙˆØ¶Ø¹ Ø£Ø­Ù…Ø± Ø´ÙØ§Ù‡.\",\n",
        "        \"PlayingPiano\": \"Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠÙØ¸Ù‡Ø± Ø´Ø®ØµÙ‹Ø§ ÙŠØ¹Ø²Ù Ø¹Ù„Ù‰ Ø¢Ù„Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ùˆ.\",\n",
        "        \"TrampolineJumping\": \"Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø´Ø®Øµ ÙŠÙ‚ÙØ² Ø¹Ù„Ù‰ Ø§Ù„ØªØ±Ø§Ù…Ø¨ÙˆÙ„ÙŠÙ†.\",\n",
        "        \"ShavingBeard\": \"Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠÙØ¸Ù‡Ø± Ø¹Ù…Ù„ÙŠØ© Ø­Ù„Ø§Ù‚Ø© Ø§Ù„Ù„Ø­ÙŠØ©.\"\n",
        "    }\n",
        "\n",
        "    return f\"ğŸ¬ Ø§Ù„ØªØµÙ†ÙŠÙ: {predicted_class}\", explanations[predicted_class], f\"ğŸ”¹ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©: {confidence:.2f} / 100\"\n",
        "\n",
        "# -------------------------------\n",
        "# Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø© Gradio\n",
        "# -------------------------------\n",
        "interface = gr.Interface(\n",
        "    fn=classify_video,\n",
        "    inputs=gr.Video(label=\"ğŸ“¹ Ø§Ø±ÙØ¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù‡Ù†Ø§\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"ğŸ” Ø§Ù„Ù†ØªÙŠØ¬Ø©\"),\n",
        "        gr.Textbox(label=\"ğŸ“ Ø§Ù„Ø´Ø±Ø­\"),\n",
        "        gr.Textbox(label=\"ğŸ“Š Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©\")\n",
        "    ],\n",
        "    title=\"ğŸ¥ ØªØµÙ†ÙŠÙ Ø£Ù†Ø´Ø·Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BLIP\",\n",
        "    description=\"Ø§Ø±ÙØ¹ ÙÙŠØ¯ÙŠÙˆ Ù‚ØµÙŠØ± (â‰¤10 Ø«ÙˆØ§Ù†ÙŠ). Ø³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª ÙˆØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ø´Ø§Ø· Ø¯Ø§Ø®Ù„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©.\"\n",
        ")\n",
        "\n",
        "interface.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "57hh0JcimN9P",
        "outputId": "7244a9ce-596a-476e-bec0-1af0878fe99c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ–¥ï¸ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: cuda\n"
          ]
        },
        {
          "ename": "HFValidationError",
          "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/UCF101_project/blip_finetunneed'. Use `repo_type` argument if needed.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    480\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/UCF101_project/blip_finetunneed'. Use `repo_type` argument if needed.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2249918503.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlipProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Salesforce/blip-image-captioning-base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mblip_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlipForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"ApplyLipstick\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PlayingPiano\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrampolineJumping\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ShavingBeard\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4732\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4733\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4734\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m   4735\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4736\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;31m# Now we try to recover if we can find all files correctly in the cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         resolved_files = [\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0m_get_cache_file_to_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         ]\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m_get_cache_file_to_return\u001b[0;34m(path_or_repo_id, full_filename, cache_dir, revision, repo_type)\u001b[0m\n\u001b[1;32m    141\u001b[0m ):\n\u001b[1;32m    142\u001b[0m     \u001b[0;31m# We try to see if we have a cached version (not up to date):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     resolved_file = try_to_load_from_cache(\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         ):\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;34mf\" '{repo_id}'. Use `repo_type` argument if needed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/UCF101_project/blip_finetunneed'. Use `repo_type` argument if needed."
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# ğŸ”Ÿ ÙˆØ§Ø¬Ù‡Ø© Gradio Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø¯Ø±Ø¨\n",
        "# ========================================\n",
        "import gradio as gr\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "# ----------------------------------------\n",
        "# ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬\n",
        "# ----------------------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"ğŸ–¥ï¸ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:\", device)\n",
        "\n",
        "# Ù†ÙØ³ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø°ÙŠ Ø§Ø³ØªØ®Ø¯Ù…ØªÙ‡ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n",
        "save_model_path = \"/content/drive/MyDrive/UCF101_project/blip_finetunneed\"\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(save_model_path).to(device)\n",
        "\n",
        "classes = [\"ApplyLipstick\", \"PlayingPiano\", \"TrampolineJumping\", \"ShavingBeard\"]\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØµÙ†Ù Ø§Ù„Ù…Ø¯Ø±Ø¨\n",
        "dummy_frame = torch.randn(1, 3, 224, 224).to(device)\n",
        "with torch.no_grad():\n",
        "    dummy_output = blip_model.vision_model(dummy_frame).last_hidden_state.mean(dim=1)\n",
        "hidden_dim = dummy_output.size(1)\n",
        "\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(hidden_dim, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(256, len(classes))\n",
        ").to(device)\n",
        "\n",
        "classifier.load_state_dict(torch.load(os.path.join(save_model_path, \"final_classifier.pth\"), map_location=device))\n",
        "classifier.eval()\n",
        "blip_model.eval()\n",
        "\n",
        "# ----------------------------------------\n",
        "# ğŸ§° Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©\n",
        "# ----------------------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def extract_frames(video_path, max_frames=16):\n",
        "    \"\"\"ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¥Ø·Ø§Ø±Ø§Øª\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    count = 0\n",
        "    while cap.isOpened() and count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "        count += 1\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "# ----------------------------------------\n",
        "# ğŸ§® Ø¯Ø§Ù„Ø© Ø§Ù„ØªØµÙ†ÙŠÙ\n",
        "# ----------------------------------------\n",
        "def classify_video(video):\n",
        "    frames = extract_frames(video)\n",
        "    if len(frames) == 0:\n",
        "        return \"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø±Ø§Øª ÙÙŠ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ\", None, None\n",
        "\n",
        "    frames = [transform(f) for f in frames]\n",
        "    frames = torch.stack(frames).unsqueeze(0).to(device)\n",
        "    T = frames.shape[1]\n",
        "\n",
        "    video_embeds = []\n",
        "    with torch.no_grad():\n",
        "        for t in range(T):\n",
        "            img = frames[:, t]\n",
        "            # Ø¹ÙƒØ³ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n",
        "            img = img * torch.tensor([0.229, 0.224, 0.225], device=device).view(1,3,1,1)\n",
        "            img = img + torch.tensor([0.485, 0.456, 0.406], device=device).view(1,3,1,1)\n",
        "            img = img.clamp(0,1)\n",
        "\n",
        "            pixel_values = processor(\n",
        "                images=img.permute(0, 2, 3, 1).cpu().numpy(),\n",
        "                return_tensors=\"pt\",\n",
        "                do_rescale=False\n",
        "            ).pixel_values.to(device)\n",
        "\n",
        "            output = blip_model.vision_model(pixel_values).last_hidden_state.mean(dim=1)\n",
        "            video_embeds.append(output)\n",
        "\n",
        "        video_embeds = torch.stack(video_embeds).mean(dim=0)\n",
        "        logits = classifier(video_embeds)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        conf, pred = torch.max(probs, 1)\n",
        "\n",
        "    predicted_class = classes[pred.item()]\n",
        "    confidence = conf.item() * 100\n",
        "\n",
        "    explanations = {\n",
        "        \"ApplyLipstick\": \"Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠÙØ¸Ù‡Ø± Ø´Ø®ØµÙ‹Ø§ ÙŠÙ‚ÙˆÙ… Ø¨ÙˆØ¶Ø¹ Ø£Ø­Ù…Ø± Ø´ÙØ§Ù‡.\",\n",
        "        \"PlayingPiano\": \"Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠÙØ¸Ù‡Ø± Ø´Ø®ØµÙ‹Ø§ ÙŠØ¹Ø²Ù Ø¹Ù„Ù‰ Ø¢Ù„Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ùˆ.\",\n",
        "        \"TrampolineJumping\": \"Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø´Ø®Øµ ÙŠÙ‚ÙØ² Ø¹Ù„Ù‰ Ø§Ù„ØªØ±Ø§Ù…Ø¨ÙˆÙ„ÙŠÙ†.\",\n",
        "        \"ShavingBeard\": \"Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠÙØ¸Ù‡Ø± Ø¹Ù…Ù„ÙŠØ© Ø­Ù„Ø§Ù‚Ø© Ø§Ù„Ù„Ø­ÙŠØ©.\"\n",
        "    }\n",
        "\n",
        "    return f\"ğŸ¬ Ø§Ù„ØªØµÙ†ÙŠÙ: {predicted_class}\", explanations[predicted_class], f\"ğŸ“Š Ø§Ù„Ø«Ù‚Ø©: {confidence:.2f}%\"\n",
        "\n",
        "# ----------------------------------------\n",
        "# ğŸ–¥ï¸ Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø© Gradio\n",
        "# ----------------------------------------\n",
        "interface = gr.Interface(\n",
        "    fn=classify_video,\n",
        "    inputs=gr.Video(label=\"ğŸ“¹ Ø§Ø±ÙØ¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù‡Ù†Ø§\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"ğŸ” Ø§Ù„Ù†ØªÙŠØ¬Ø©\"),\n",
        "        gr.Textbox(label=\"ğŸ“ Ø§Ù„Ø´Ø±Ø­\"),\n",
        "        gr.Textbox(label=\"ğŸ“Š Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©\")\n",
        "    ],\n",
        "    title=\"ğŸ¥ ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BLIP (UCF101)\",\n",
        "    description=\"Ø§Ø±ÙØ¹ ÙÙŠØ¯ÙŠÙˆ Ù‚ØµÙŠØ± (â‰¤10 Ø«ÙˆØ§Ù†Ù). Ø³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª ÙˆØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ø´Ø§Ø·.\"\n",
        ")\n",
        "\n",
        "interface.launch(debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOnq2MMcrHHxnkx+eCsjdpw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}